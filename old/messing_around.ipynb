{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Language model fine-tuned on statements and values to provide chatbot conversations with humans with different values</b>\n",
    "\n",
    "\n",
    "1. Train a BERT (BART?) model to classify values based on prompt sentences\n",
    "2. Apply reinforcement learning using the learned value model to fine-tune language model to predict next sentence in sequence.\n",
    "3. Can we use this to create a chat bot or do we need to create our own language model and architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 10)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to do a simple forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[0.0537]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "folder_name = 'v0.3_original'\n",
    "df_ACHIEVEMENT = pd.read_csv(os.path.join(folder_name, 'ACHIEVEMENT.csv'))\n",
    "s = df_ACHIEVEMENT['scenario'][0]\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    inp = tokenizer(s, return_tensors='pt')\n",
    "    out = model(**inp)\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process dataset\n",
    "1. Load into dataframe\n",
    "2. Tokenize the data to fit input format for bert model\n",
    "\n",
    "<b>Problems</b>\n",
    "How to handle zero values? Do we need to use the value in the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "VALUES = ['ACHIEVEMENT', 'BENEVOLENCE', 'CONFORMITY', 'HEDONISM', 'POWER', 'SECURITY', 'SELF-DIRECTION', 'STIMULATION', 'TRADITION', 'UNIVERSALISM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class ValueDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer): \n",
    "        df = self._load_data(VALUES)\n",
    "        self.scenarios = df['scenario'].values.tolist()\n",
    "        self.values = df['label'].values.tolist()\n",
    "        self.N = df.shape[0]\n",
    "\n",
    "        inp = tokenizer(self.scenarios, return_tensors='pt', padding=True, truncation=True)\n",
    "        self.input_ids = inp.get('input_ids')\n",
    "        self.attention_mask = inp.get('attention_mask')\n",
    "        self.token_type_ids = inp.get('token_type_ids')\n",
    "\n",
    "        self.y = torch.from_numpy(df[VALUES].values) # Values\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.attention_mask[index], self.token_type_ids[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def _load_data(self, values):\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for value in values:\n",
    "            df_value = pd.read_csv(os.path.join(folder_name, value + '.csv'))\n",
    "            df_value = df_value.rename({'label' : value}, axis = 'columns')\n",
    "            df_value['label'] = value\n",
    "            df = pd.concat([df, df_value])\n",
    "\n",
    "        df.fillna(0, inplace=True)\n",
    "        df.reset_index(inplace=True)\n",
    "        return df\n",
    "\n",
    "dataset = ValueDataset(AutoTokenizer.from_pretrained('bert-base-uncased'))\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=10, shuffle=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 10)\n",
    "\n",
    "# Try to conduct a batched forward pass\n",
    "with torch.no_grad():\n",
    "    input_ids, attention_mask, token_types_ids, targets = next(iter(dataloader)) \n",
    "    inp = {'input_ids' : input_ids, 'attention_mask' : attention_mask, 'token_type_ids' : token_types_ids}\n",
    "    output = model(**inp)\n",
    "    assert output.logits.size() == targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(values):\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for value in values:\n",
    "            df_value = pd.read_csv(os.path.join(folder_name, value + '.csv'))\n",
    "            df_value = df_value.rename({'label' : value}, axis = 'columns')\n",
    "            df_value['label'] = value\n",
    "            df = pd.concat([df, df_value])\n",
    "\n",
    "        df.fillna(0, inplace=True)\n",
    "        df.reset_index(inplace=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "df = load_data(VALUES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = .8\n",
    "N = df.shape[0]\n",
    "\n",
    "train_dataset = Dataset.from_dict(tokenizer(df['scenario'][0:int(N * train_split)].values.tolist(), padding = True, truncation = True, return_tensors = 'pt'))\n",
    "validation_dataset = Dataset.from_dict(tokenizer(df['scenario'][int(N * train_split):].values.tolist(), padding = True, truncation = True, return_tensors = 'pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "1. Train model to score values based on different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                             0\n",
       "Unnamed: 0                                                        0\n",
       "uid                                                            2635\n",
       "scenario          Tomorrow I will audition my singing in a talen...\n",
       "ACHIEVEMENT                                                     1.0\n",
       "label                                                   ACHIEVEMENT\n",
       "BENEVOLENCE                                                     0.0\n",
       "CONFORMITY                                                      0.0\n",
       "HEDONISM                                                        0.0\n",
       "POWER                                                           0.0\n",
       "SECURITY                                                        0.0\n",
       "SELF-DIRECTION                                                  0.0\n",
       "STIMULATION                                                     0.0\n",
       "TRADITION                                                       0.0\n",
       "UNIVERSALISM                                                    0.0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "AutoModelForSequenceClassification is designed to be instantiated using the `AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)` or `AutoModelForSequenceClassification.from_config(config)` methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, load_dataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#Y100sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForSequenceClassification(\u001b[39m'\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m, num_labels \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m), \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#Y100sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_dataset \u001b[39m=\u001b[39m train_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#Y100sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     eval_dataset \u001b[39m=\u001b[39m validation_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#Y100sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m tokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#Y100sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:411\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 411\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    412\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is designed to be instantiated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39musing the `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.from_pretrained(pretrained_model_name_or_path)` or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    414\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.from_config(config)` methods.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: AutoModelForSequenceClassification is designed to be instantiated using the `AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)` or `AutoModelForSequenceClassification.from_config(config)` methods."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = AutoModelForSequenceClassification('bert-base-uncased', num_labels = 10), \n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = validation_dataset,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5816, -0.1002, -0.8615, -0.3651,  0.3426,  0.1915,  0.4009,  0.2929,\n",
       "          0.1102, -0.0562],\n",
       "        [ 0.6290, -0.2749, -0.5745, -0.1988,  0.1777,  0.0832,  0.2657,  0.2745,\n",
       "          0.0169,  0.0462],\n",
       "        [ 0.5651,  0.0624, -0.7030, -0.1752,  0.1315, -0.0870, -0.0310,  0.2596,\n",
       "         -0.0729,  0.1607],\n",
       "        [ 0.6077, -0.0241, -0.7932, -0.3335,  0.3101,  0.1662,  0.3302,  0.3789,\n",
       "          0.0865,  0.0444],\n",
       "        [ 0.6952, -0.0650, -0.9161, -0.2959,  0.3510,  0.1231,  0.2523,  0.3453,\n",
       "          0.0801,  0.1204],\n",
       "        [ 0.6582, -0.2365, -0.7176, -0.3096,  0.2731,  0.0280,  0.1341,  0.2272,\n",
       "         -0.0747,  0.1925],\n",
       "        [ 0.6931, -0.1226, -0.7247, -0.2466,  0.2624, -0.0261,  0.1203,  0.2805,\n",
       "         -0.1136,  0.1546],\n",
       "        [ 0.6367, -0.1960, -0.5345, -0.1218,  0.2135, -0.0971,  0.2804,  0.2762,\n",
       "         -0.0388, -0.0614],\n",
       "        [ 0.6698, -0.0232, -0.8080, -0.2781,  0.3113,  0.0392,  0.2370,  0.2740,\n",
       "          0.0713,  0.0606],\n",
       "        [ 0.6423, -0.1436, -0.5865, -0.1700,  0.2600, -0.0563,  0.1621,  0.1471,\n",
       "         -0.0264,  0.0520]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(out\u001b[39m.\u001b[39mlogits, targets)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sigurdfrankthorlundnielsen/Documents/University/MSc/nlp/project/nlp-project/messing_around.ipynb#X56sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "loss_function = F.mse_loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    input_ids, attention_mask, token_types_ids, targets = next(iter(dataloader)) \n",
    "    inp = {'input_ids' : input_ids, 'attention_mask' : attention_mask, 'token_type_ids' : token_types_ids}\n",
    "    out = model(**inp)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(out.logits, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
